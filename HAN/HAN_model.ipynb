{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41a4e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2022.11.15，复现HAN(Heterogeneous Graph Attention Network)\\ndependencies:\\n    tensorflow-2.11.0\\n    numpy-1.22.0\\n    networkx-2.6.3\\n    scipy-1.7.3\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2022.11.15，复现HAN(Heterogeneous Graph Attention Network)\n",
    "dependencies:\n",
    "    tensorflow-2.11.0\n",
    "    numpy-1.22.0\n",
    "    networkx-2.6.3\n",
    "    scipy-1.7.3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7c1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "project_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a83d679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\PycharmProjects\\\\GNN Algorithms\\\\HAN'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87dee6",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f61b77",
   "metadata": {},
   "source": [
    "## data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca23177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yysgz\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e4615c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Prepare adjacency matrix by expanding up to a given neighbourhood.\\n This will insert loops on every node.\\n Finally, the matrix is converted to bias vectors.\\n Expected shape: [graph, nodes, nodes]\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " Prepare adjacency matrix by expanding up to a given neighbourhood.\n",
    " This will insert loops on every node.\n",
    " Finally, the matrix is converted to bias vectors.\n",
    " Expected shape: [graph, nodes, nodes]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1903d",
   "metadata": {},
   "source": [
    "### adj_to_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19132eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3,4)[0] + np.eye(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a260b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_to_bias(adj, sizes, nhood=1):  # 邻接矩阵adjacency matrix\n",
    "    nb_graphs = adj.shape[0]  # 行,即num_nodes\n",
    "    mt = np.empty(adj.shape)  # 根据给定的维度和数值类型，返回一个新的ndarray数组，其元素不进行初始化\n",
    "    for g in range(nb_graphs):\n",
    "        mt[g] = np.eye(adj.shape[1])  # 返回一个二维的ndarray数组\n",
    "        for _ in range(nhood):\n",
    "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))  # 相乘\n",
    "        for i in range(sizes[g]):  # 这个应该可以简化，直接对整个数组元素做操作！！！\n",
    "            for j in range(sizes[g]):\n",
    "                if mt[g][i][j] > 0.0:\n",
    "                    mt[g][i][j] = 1.0\n",
    "    return -1e9 * (1.0 - mt)  # 科学计数法，2.5 x 10^(-27)表示为：2.5e-27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485de35a",
   "metadata": {},
   "source": [
    "###  parse_index_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce8a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5905152",
   "metadata": {},
   "source": [
    "### sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573400ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成掩码bool数组\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)  # 生成全是0的数组\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18145aa",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f12d9a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data(dataset_str):  # {'pubmed', 'citeseer', 'cora'}\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:  # 此人编码功底实在很烂\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))  # 序列化读出file对象\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder) + 1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))  # 创建一个空的lil_matrix\n",
    "        tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))  # 创建一个shape的全是0的数组\n",
    "        ty_extended[test_idx_range - min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()  # vstack按行拼接\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))  # 从列表字典中返回一个图，获取邻接矩阵\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + 500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])  # 生成掩码bool数组\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)  # 全为0的shape numpy数组\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    print(adj.shape)\n",
    "    print(features.shape)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170368d",
   "metadata": {},
   "source": [
    "### sparse_to_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3658f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''转换为稀疏矩阵tuple'''\n",
    "def to_tuple(mx):\n",
    "    if not sp.isspmatrix_coo(mx):\n",
    "        mx = mx.tocoo()\n",
    "    coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "    values = mx.data\n",
    "    shape = mx.shape\n",
    "    return coords, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a14574b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e90dc9",
   "metadata": {},
   "source": [
    "### standardize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d379e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(f, train_mask):\n",
    "    \"\"\"Standardize feature matrix and convert to tuple representation\"\"\"\n",
    "    # standardize data\n",
    "    f = f.todense()  # 将稀疏矩阵转回numpy矩阵\n",
    "    mu = f[train_mask == True, :].mean(axis=0)  # 按行求平均\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = f[:, np.squeeze(np.array(sigma > 0))]  # sigma>0 get bool array\n",
    "    mu = f[train_mask == True, :].mean(axis=0)\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = (f - mu) / sigma\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e8000",
   "metadata": {},
   "source": [
    "### preprocess_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1162540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()  # power数组元素求n次方，flatten是降到一维\n",
    "    r_inv[np.isinf(r_inv)] = 0.  # isinf判断是否为无穷\n",
    "    r_mat_inv = sp.diags(r_inv)  # 从对角线构造一个稀疏矩阵。\n",
    "    features = r_mat_inv.dot(features)  # dot矩阵乘法\n",
    "    return features.todense(), sparse_to_tuple(features)  # todense()转换成密集矩阵numpy.matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d23fc",
   "metadata": {},
   "source": [
    "### normalize_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc2a875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix. 对称归一化邻接矩阵\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996acf6",
   "metadata": {},
   "source": [
    "### preprocess_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16124e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))  # 对角线为1的矩阵\n",
    "    return sparse_to_tuple(adj_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595f759",
   "metadata": {},
   "source": [
    "## layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "767be59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_slim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94785fad",
   "metadata": {},
   "source": [
    "### attn_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4824f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_head(features, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False,\n",
    "              return_coef=False):\n",
    "    \"\"\"[summary]\n",
    "    multi-head attention计算\n",
    "    [description]\n",
    "    # forward；model = HeteGAT_multi\n",
    "    attns.append(layers.attn_head(features,            # list:3, tensor（1， 3025， 1870）\n",
    "                                bias_mat=bias_mat,     # list:2, tensor(1, 3025, 3025)\n",
    "                                out_sz=hid_units[0],   # hid_units:[8]，卷积核的个数\n",
    "                                activation=activation, # nonlinearity:tf.nn.elu\n",
    "                                in_drop=ffd_drop,      # tensor, ()\n",
    "                                coef_drop=attn_drop,   # tensor, ()\n",
    "                                residual=False))\n",
    "    Arguments:\n",
    "        features {[type]} -- shape=(batch_size, nb_nodes, fea_size))\n",
    "    \"\"\"\n",
    "    with tf.name_scope('my_attn'):  # 定义一个上下文管理器\n",
    "        if in_drop != 0.0:\n",
    "            features = tf.nn.dropout(features, 1.0 - in_drop)  # 以rate置0\n",
    "        features_fts = tf.compat.v1.layers.conv1d(features, out_sz, 1, use_bias=False)  # 一维卷积操作, out: (1, 3025, 8)\n",
    "        \n",
    "        f_1 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
    "        f_2 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
    "        \n",
    "        logits = f_1 + tf.transpose(f_2, [0, 2, 1])  # 转置         # (1, 3025, 3025)\n",
    "        coefs = tf.nn.softmax(tf.nn.leaky_relu(logits) + bias_mat)  # (1, 3025, 3025)\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)           \n",
    "        if in_drop != 0.0:\n",
    "            features_fts = tf.nn.dropout(features_fts, 1.0 - in_drop)\n",
    "\n",
    "        vals = tf.matmul(coefs, features_fts)                       # (1, 3025, 8)\n",
    "        ret = tf_slim.bias_add(vals)  # 将bias向量加到value矩阵上      # (1. 3025， 8)\n",
    "\n",
    "        # residual connection 残差连接\n",
    "        if residual:\n",
    "            if features.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + tf.compat.v1.layers.conv1d(features, ret.shape[-1], 1)  # activation\n",
    "            else:\n",
    "                features_fts = ret + features\n",
    "        if return_coef:\n",
    "            return activation(ret), coefs\n",
    "        else:\n",
    "            return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b21e1",
   "metadata": {},
   "source": [
    "### attn_head_const_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22b7c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_head_const_1(seq, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False):\n",
    "    \"\"\"[summary]\n",
    "    [description]\n",
    "    \"\"\"\n",
    "    adj_mat = 1.0 - bias_mat / -1e9\n",
    "    with tf.name_scope('my_attn'):\n",
    "        if in_drop != 0.0:\n",
    "            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
    "        seq_fts = tf.compat.v1.layers.conv1d(seq, out_sz, 1, use_bias=False)\n",
    "        \n",
    "\n",
    "        logits = adj_mat \n",
    "        coefs = tf.nn.softmax(tf.nn.leaky_relu(logits) + bias_mat)\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)\n",
    "        if in_drop != 0.0:\n",
    "            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n",
    "\n",
    "        vals = tf.matmul(coefs, seq_fts)\n",
    "        ret = tf_slim.bias_add(vals)\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + tf.compat.v1.layers.conv1d(seq, ret.shape[-1], 1)  # activation\n",
    "            else:\n",
    "                seq_fts = ret + seq\n",
    "\n",
    "        return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a977a72",
   "metadata": {},
   "source": [
    "### sp_attn_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6ac3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_attn_head(seq, out_sz, adj_mat, activation, nb_nodes, in_drop=0.0, coef_drop=0.0, residual=False):\n",
    "    with tf.name_scope('sp_attn'):\n",
    "        if in_drop != 0.0:\n",
    "            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
    "\n",
    "        seq_fts = tf.compat.v1.layers.conv1d(seq, out_sz, 1, use_bias=False)\n",
    "\n",
    "        # simplest self-attention possible\n",
    "        f_1 = tf.compat.v1.layers.conv1d(seq_fts, 1, 1)\n",
    "        f_2 = tf.compat.v1.layers.conv1d(seq_fts, 1, 1)\n",
    "        logits = tf.sparse_add(adj_mat * f_1, adj_mat *\n",
    "                               tf.transpose(f_2, [0, 2, 1]))\n",
    "        lrelu = tf.SparseTensor(indices=logits.indices,\n",
    "                                values=tf.nn.leaky_relu(logits.values),\n",
    "                                dense_shape=logits.dense_shape)\n",
    "        coefs = tf.sparse_softmax(lrelu)  # 将softmax应用于批量的N维SparseTensor\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.SparseTensor(indices=coefs.indices,\n",
    "                                    values=tf.nn.dropout(\n",
    "                                        coefs.values, 1.0 - coef_drop),\n",
    "                                    dense_shape=coefs.dense_shape)\n",
    "        if in_drop != 0.0:\n",
    "            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n",
    "\n",
    "        # As tf.sparse_tensor_dense_matmul expects its arguments to have rank-2,\n",
    "        # here we make an assumption that our input is of batch size 1, and reshape appropriately.\n",
    "        # The method will fail in all other cases!\n",
    "        coefs = tf.sparse_reshape(coefs, [nb_nodes, nb_nodes])\n",
    "        seq_fts = tf.squeeze(seq_fts)  \n",
    "        vals = tf.sparse_tensor_dense_matmul(coefs, seq_fts)  # SparseTensor稀疏矩阵乘法\n",
    "        vals = tf.expand_dims(vals, axis=0)  # 在0处扩展维度1\n",
    "        vals.set_shape([1, nb_nodes, out_sz])\n",
    "        ret = tf_slim.bias_add(vals)\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + tf.compat.v1.layers.conv1d(seq, ret.shape[-1], 1)  # activation\n",
    "            else:\n",
    "                seq_fts = ret + seq\n",
    "\n",
    "        return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ae2a3",
   "metadata": {},
   "source": [
    "### SimpleAttLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff4a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_embed, att_val = layers.SimpleAttLayer(multi_embed, mp_att_size,\n",
    "#                                                      time_major=False,\n",
    "#                                                      return_alphas=True)\n",
    "def SimpleAttLayer(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    '''\n",
    "    inputs: tensor, (3025, 2, 64)\n",
    "    attention_size: 128\n",
    "    '''\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)  # 表示在shape第2个维度上拼接\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])  #\n",
    "\n",
    "    hidden_size = inputs.shape[2]  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.compat.v1.random_normal([hidden_size, attention_size], stddev=0.1))  # (64, 128)\n",
    "    b_omega = tf.Variable(tf.compat.v1.random_normal([attention_size], stddev=0.1))               # (128, )\n",
    "    u_omega = tf.Variable(tf.compat.v1.random_normal([attention_size], stddev=0.1))               # (128, )\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)   # (3025, 2, 128)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape   tensor, (3025, 2)\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape   tensor, (3025, 2)\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)  # (3025, 2, 64) * (3025, 2, 1) = (3025, 2, 64) -> (3025, 2)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas  # attention输出、softmax概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72795e7",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15b06dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle  # 把训练好的模型存储起来\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from sklearn import manifold  # 一种非线性降维的手段\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec7c941",
   "metadata": {},
   "source": [
    "## KNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ab62687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_KNN(x, y, k=5, split_list=[0.2, 0.4, 0.6, 0.8], time=10, show_train=True, shuffle=True):\n",
    "    x = np.array(x)\n",
    "    x = np.squeeze(x)\n",
    "    y = np.array(y)\n",
    "    if len(y.shape) > 1:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    for split in split_list:\n",
    "        ss = split\n",
    "        split = int(x.shape[0] * split)\n",
    "        micro_list = []\n",
    "        macro_list = []\n",
    "        if time:\n",
    "            for i in range(time):\n",
    "                if shuffle:\n",
    "                    permutation = np.random.permutation(x.shape[0])  # 生成一个随机打散的序列。\n",
    "                    x = x[permutation, :]\n",
    "                    y = y[permutation]\n",
    "                # x_true = np.array(x_true)\n",
    "                train_x = x[:split, :]\n",
    "                test_x = x[split:, :]\n",
    "\n",
    "                train_y = y[:split]\n",
    "                test_y = y[split:]\n",
    "\n",
    "                estimator = KNeighborsClassifier(n_neighbors=k)\n",
    "                estimator.fit(train_x, train_y)\n",
    "                y_pred = estimator.predict(test_x)\n",
    "                f1_macro = f1_score(test_y, y_pred, average='macro')\n",
    "                f1_micro = f1_score(test_y, y_pred, average='micro')\n",
    "                macro_list.append(f1_macro)\n",
    "                micro_list.append(f1_micro)\n",
    "            print('KNN({}avg, split:{}, k={}) f1_macro: {:.4f}, f1_micro: {:.4f}'.format(\n",
    "                time, ss, k, sum(macro_list) / len(macro_list), sum(micro_list) / len(micro_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728cdedd",
   "metadata": {},
   "source": [
    "## kmeans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5798d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_Kmeans(x, y, k=4, time=10, return_NMI=False):\n",
    "\n",
    "    x = np.array(x)\n",
    "    x = np.squeeze(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if len(y.shape) > 1:\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    estimator = KMeans(n_clusters=k)\n",
    "    ARI_list = []  # adjusted_rand_score(\n",
    "    NMI_list = []\n",
    "    if time:\n",
    "        # print('KMeans exps {}次 æ±~B平å~]~G '.format(time))\n",
    "        for i in range(time):\n",
    "            estimator.fit(x, y)\n",
    "            y_pred = estimator.predict(x)\n",
    "            score = normalized_mutual_info_score(y, y_pred)\n",
    "            NMI_list.append(score)\n",
    "            s2 = adjusted_rand_score(y, y_pred)\n",
    "            ARI_list.append(s2)\n",
    "        # print('NMI_list: {}'.format(NMI_list))\n",
    "        score = sum(NMI_list) / len(NMI_list)\n",
    "        s2 = sum(ARI_list) / len(ARI_list)\n",
    "        print('NMI (10 avg): {:.4f} , ARI (10avg): {:.4f}'.format(score, s2))\n",
    "\n",
    "    else:\n",
    "        estimator.fit(x, y)\n",
    "        y_pred = estimator.predict(x)\n",
    "        score = normalized_mutual_info_score(y, y_pred)\n",
    "        print(\"NMI on all label data: {:.5f}\".format(score))\n",
    "    if return_NMI:\n",
    "        return score, s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69896b23",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d1fbd",
   "metadata": {},
   "source": [
    "## BaseGAttN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0fb822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "var = tf.Variable(np.random.random(size=(1,)))\n",
    "print(type([var]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "873432fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecedc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGAttN:\n",
    "    def loss(logits, labels, nb_classes, class_weights):\n",
    "        sample_wts = tf.reduce_sum(tf.multiply(\n",
    "            tf.one_hot(labels, nb_classes), class_weights), axis=-1)\n",
    "        xentropy = tf.multiply(tf.nn.sparse_softmax_cross_entropy_with_logits(  # 计算多分类交叉熵\n",
    "            labels=labels, logits=logits), sample_wts)\n",
    "        return tf.reduce_mean(xentropy, name='xentropy_mean')\n",
    "    \n",
    "    # 更新梯度权重\n",
    "    def training(loss, lr, l2_coef):\n",
    "        # weight decay\n",
    "        vars = tf.compat.v1.trainable_variables()  # 查看可训练变量,list\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not    # add_n实现列表相加；l2_loss是l2范数值得一半\n",
    "                           in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef\n",
    "        # lossL2, tensor(mul_5.0), shape=()\n",
    "        # optimizer\n",
    "        opt = tf.compat.v1.train.AdamOptimizer(learning_rate=lr)  # adam函数\n",
    "\n",
    "        # training op\n",
    "        train_op = opt.minimize(loss + lossL2)  # 计算梯度，然后更新参数\n",
    "\n",
    "        return train_op\n",
    "    \n",
    "    def preshape(logits, labels, nb_classes):\n",
    "        new_sh_lab = [-1]\n",
    "        new_sh_log = [-1, nb_classes]\n",
    "        log_resh = tf.reshape(logits, new_sh_log)\n",
    "        lab_resh = tf.reshape(labels, new_sh_lab)\n",
    "        return log_resh, lab_resh\n",
    "\n",
    "    def confmat(logits, labels):\n",
    "        preds = tf.argmax(logits, axis=1)  # 返回行最大值索引\n",
    "        return tf.confusion_matrix(labels, preds)  # 混淆矩阵\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    # Adapted from tkipf/gcn #\n",
    "    ##########################\n",
    "\n",
    "    def masked_softmax_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(  # 返回交叉熵向量\n",
    "            logits=logits, labels=labels)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)  # 改变tensor数据类型\n",
    "        mask /= tf.reduce_mean(mask)  # 通过均值求loss\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def masked_sigmoid_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        labels = tf.cast(labels, dtype=tf.float32)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(  # sigmoid交叉熵\n",
    "            logits=logits, labels=labels)\n",
    "        loss = tf.reduce_mean(loss, axis=1)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def masked_accuracy(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        correct_prediction = tf.equal(   # 判断向量元素是否相等\n",
    "            tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        accuracy_all *= mask\n",
    "        return tf.reduce_mean(accuracy_all)\n",
    "    \n",
    "    def micro_f1(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        predicted = tf.round(tf.nn.sigmoid(logits))  # 四舍五入函数\n",
    "\n",
    "        # Use integers to avoid any nasty FP behaviour\n",
    "        predicted = tf.cast(predicted, dtype=tf.int32)\n",
    "        labels = tf.cast(labels, dtype=tf.int32)\n",
    "        mask = tf.cast(mask, dtype=tf.int32)\n",
    "\n",
    "        # expand the mask so that broadcasting works ([nb_nodes, 1])\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "\n",
    "        # Count true positives, true negatives, false positives and false negatives.\n",
    "        tp = tf.count_nonzero(predicted * labels * mask)  # 非零元素个数\n",
    "        tn = tf.count_nonzero((predicted - 1) * (labels - 1) * mask)\n",
    "        fp = tf.count_nonzero(predicted * (labels - 1) * mask)\n",
    "        fn = tf.count_nonzero((predicted - 1) * labels * mask)\n",
    "\n",
    "        # Calculate accuracy, precision, recall and F1 score.\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "        fmeasure = tf.cast(fmeasure, tf.float32)\n",
    "        return fmeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9512b",
   "metadata": {},
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b5afdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f6d2f",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0cd1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(BaseGAttN):\n",
    "    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "                  bias_mat, hid_units, n_heads, activation=tf.nn.elu, residual=False):  # 残差\n",
    "        attns = []\n",
    "        for _ in range(n_heads[0]):\n",
    "            attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                                          out_sz=hid_units[0], activation=activation,\n",
    "                                          in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        h_1 = tf.concat(attns, axis=-1)\n",
    "        # multi-head attention\n",
    "        for i in range(1, len(hid_units)):\n",
    "            h_old = h_1\n",
    "            attns = []\n",
    "            for _ in range(n_heads[i]):\n",
    "                attns.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                                              out_sz=hid_units[i], activation=activation,\n",
    "                                              in_drop=ffd_drop, coef_drop=attn_drop, residual=residual))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                                        out_sz=nb_classes, activation=lambda x: x,\n",
    "                                        in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a3622",
   "metadata": {},
   "source": [
    "### HeteGAT_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64137920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteGAT_multi(BaseGAttN):\n",
    "    '''\n",
    "    # forward；model = HeteGAT_multi\n",
    "    logits, final_embedding, att_val = model.inference(ftr_in_list,  # list:3, tensor（1， 3025， 1870）\n",
    "                                                       nb_classes,   # 3\n",
    "                                                       nb_nodes,     # 3025\n",
    "                                                       is_train,     # bool\n",
    "                                                       attn_drop,    # tensor, ()\n",
    "                                                       ffd_drop,     # tensor, ()\n",
    "                                                       bias_mat_list=bias_in_list,  # list:2, tensor()\n",
    "                                                       hid_units=hid_units,   # hid_units:8\n",
    "                                                       n_heads=n_heads,       # n_heads: [8, 1]\n",
    "                                                       residual=residual,     # residual: False\n",
    "                                                       activation=nonlinearity)  # nonlinearity:tf.nn.elu\n",
    "\n",
    "    '''\n",
    "    def inference(ftr_in_list, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "                  bias_mat_list, hid_units, n_heads, activation=tf.nn.elu, residual=False,\n",
    "                  mp_att_size=128):\n",
    "        embed_list = []\n",
    "        for features, bias_mat in zip(ftr_in_list, bias_mat_list):\n",
    "            attns = []\n",
    "            jhy_embeds = []\n",
    "            for _ in range(n_heads[0]):   # [8,1]\n",
    "                # multi-head attention 计算\n",
    "                attns.append(attn_head(features, bias_mat=bias_mat,\n",
    "                                              out_sz=hid_units[0], activation=activation,\n",
    "                                              in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "\n",
    "            for i in range(1, len(hid_units)):\n",
    "                h_old = h_1\n",
    "                attns = []\n",
    "                for _ in range(n_heads[i]):\n",
    "                    attns.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  in_drop=ffd_drop,\n",
    "                                                  coef_drop=attn_drop, residual=residual))\n",
    "                h_1 = tf.concat(attns, axis=-1)\n",
    "            embed_list.append(tf.expand_dims(tf.squeeze(h_1), axis=1))  # list:2. 其中每个元素tensor, (3025, 1, 64)\n",
    "\n",
    "        multi_embed = tf.concat(embed_list, axis=1)   # tensor, (3025, 2, 64)\n",
    "        # attention输出：tensor(3025, 64)、softmax概率\n",
    "        final_embed, att_val = SimpleAttLayer(multi_embed, \n",
    "                                              mp_att_size,\n",
    "                                              time_major=False,\n",
    "                                              return_alphas=True)\n",
    "\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):  # 1\n",
    "            # 用于添加一个全连接层(input, output) -> (3025, 3)\n",
    "            out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))  \n",
    "        #     out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "        #                                 out_sz=nb_classes, activation=lambda x: x,\n",
    "        #                                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]  # add_n是列表相加。tensor,(3025, 3)\n",
    "        # logits_list.append(logits)\n",
    "        print('de')\n",
    "\n",
    "        logits = tf.expand_dims(logits, axis=0)  # (1, 3025, 3)\n",
    "        # attention通过全连接层预测(1, 3025, 3)、attention final_embedding tensor(3025, 64)、attention 概率\n",
    "        return logits, final_embed, att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7de40d",
   "metadata": {},
   "source": [
    "### HeteGAT_no_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f795a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteGAT_no_coef(BaseGAttN):\n",
    "    def inference(ftr_in_list, nb_classes, nb_nodes, is_train, attn_drop, ffd_drop,\n",
    "                  bias_mat_list, hid_units, n_heads, activation=tf.nn.elu, residual=False,\n",
    "                  mp_att_size=128):\n",
    "        embed_list = []\n",
    "        # coef_list = []\n",
    "        for bias_mat in bias_mat_list:\n",
    "            attns = []\n",
    "            head_coef_list = []\n",
    "            for _ in range(n_heads[0]):\n",
    "\n",
    "                attns.append(attn_head(ftr_in_list, bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[0], activation=activation,\n",
    "                                                  in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                                                  return_coef=return_coef))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "            for i in range(1, len(hid_units)):\n",
    "                h_old = h_1\n",
    "                attns = []\n",
    "                for _ in range(n_heads[i]):\n",
    "                    attns.append(attn_head(h_1,\n",
    "                                                  bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  in_drop=ffd_drop,\n",
    "                                                  coef_drop=attn_drop,\n",
    "                                                  residual=residual))\n",
    "                h_1 = tf.concat(attns, axis=-1)\n",
    "            embed_list.append(tf.expand_dims(tf.squeeze(h_1), axis=1))\n",
    "        # att for metapath\n",
    "        # prepare shape for SimpleAttLayer\n",
    "        # print('att for mp')\n",
    "        multi_embed = tf.concat(embed_list, axis=1)\n",
    "        final_embed, att_val = SimpleAttLayer(multi_embed, mp_att_size,\n",
    "                                                     time_major=False,\n",
    "                                                     return_alphas=True)\n",
    "        # print(att_val)\n",
    "        # last layer for clf\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))\n",
    "        #     out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "        #                                 out_sz=nb_classes, activation=lambda x: x,\n",
    "        #                                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "        # logits_list.append(logits)\n",
    "        print('de')\n",
    "        logits = tf.expand_dims(logits, axis=0)\n",
    "        # if return_coef:\n",
    "        #     return logits, final_embed, att_val, coef_list\n",
    "        # else:\n",
    "        return logits, final_embed, att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c6f62",
   "metadata": {},
   "source": [
    "### HeteGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4586445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteGAT(BaseGAttN):\n",
    "    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "                  bias_mat_list, hid_units, n_heads, activation=tf.nn.elu, residual=False,\n",
    "                  mp_att_size=128,\n",
    "                  return_coef=False):\n",
    "        embed_list = []\n",
    "        coef_list = []\n",
    "        for bias_mat in bias_mat_list:\n",
    "            attns = []\n",
    "            head_coef_list = []\n",
    "            for _ in range(n_heads[0]):\n",
    "                if return_coef:\n",
    "                    a1, a2 = attn_head(inputs, bias_mat=bias_mat,\n",
    "                                              out_sz=hid_units[0], activation=activation,\n",
    "                                              in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                                              return_coef=return_coef)\n",
    "                    attns.append(a1)\n",
    "                    head_coef_list.append(a2)\n",
    "                    # attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                    #                               out_sz=hid_units[0], activation=activation,\n",
    "                    #                               in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                    #                               return_coef=return_coef)[0])\n",
    "                    #\n",
    "                    # head_coef_list.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                    #                                        out_sz=hid_units[0], activation=activation,\n",
    "                    #                                        in_drop=ffd_drop, coef_drop=attn_drop,\n",
    "                    #                                        residual=False,\n",
    "                    #                                        return_coef=return_coef)[1])\n",
    "                else:\n",
    "                    attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[0], activation=activation,\n",
    "                                                  in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                                                  return_coef=return_coef))\n",
    "            head_coef = tf.concat(head_coef_list, axis=0)\n",
    "            head_coef = tf.reduce_mean(head_coef, axis=0)\n",
    "            coef_list.append(head_coef)\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "            for i in range(1, len(hid_units)):\n",
    "                h_old = h_1\n",
    "                attns = []\n",
    "                for _ in range(n_heads[i]):\n",
    "                    attns.append(attn_head(h_1,\n",
    "                                                  bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  in_drop=ffd_drop,\n",
    "                                                  coef_drop=attn_drop,\n",
    "                                                  residual=residual))\n",
    "                h_1 = tf.concat(attns, axis=-1)\n",
    "            embed_list.append(tf.expand_dims(tf.squeeze(h_1), axis=1))\n",
    "        # att for metapath\n",
    "        # prepare shape for SimpleAttLayer\n",
    "        # print('att for mp')\n",
    "        multi_embed = tf.concat(embed_list, axis=1)\n",
    "        final_embed, att_val = SimpleAttLayer(multi_embed, mp_att_size,\n",
    "                                                     time_major=False,\n",
    "                                                     return_alphas=True)\n",
    "        # print(att_val)\n",
    "        # last layer for clf\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))\n",
    "        #     out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "        #                                 out_sz=nb_classes, activation=lambda x: x,\n",
    "        #                                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "        # logits_list.append(logits)\n",
    "        logits = tf.expand_dims(logits, axis=0)\n",
    "        if return_coef:\n",
    "            return logits, final_embed, att_val, coef_list\n",
    "        else:\n",
    "            return logits, final_embed, att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59adf56d",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3b4f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'  # 设置使用GPU1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c4e7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()  # 用来对session进行参数配置\n",
    "config.gpu_options.allow_growth = True  # 允许tf自动选择一个存在并且可用的设备来运行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74b23b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: D:\\PycharmProjects\\GNN Algorithms/data/han_data/acm/acm_allMP_multi_fea_.ckpt\n",
      "Dataset: acm\n",
      "----- Opt. hyperparams -----\n",
      "lr: 0.005\n",
      "l2_coef: 0.001\n",
      "----- Archi. hyperparams -----\n",
      "nb. layers: 1\n",
      "nb. units per layer: [8]\n",
      "nb. attention heads: [8, 1]\n",
      "residual: False\n",
      "nonlinearity: <function elu at 0x000001997B93C1F0>\n",
      "model: <class '__main__.HeteGAT_multi'>\n"
     ]
    }
   ],
   "source": [
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "checkpt_file = os.path.abspath(os.path.dirname(os.getcwd())) +\\\n",
    "                            '/data/han_data/acm_features.ckpt'\n",
    "print('model: {}'.format(checkpt_file))\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 200\n",
    "patience = 100\n",
    "lr = 0.005  # learning rate\n",
    "l2_coef = 0.001  # weight decay\n",
    "# numbers of hidden units per each attention head in each layer\n",
    "hid_units = [8]\n",
    "n_heads = [8, 1]  # additional entry for the output layer\n",
    "residual = False\n",
    "nonlinearity = tf.nn.elu\n",
    "model = HeteGAT_multi\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "print('lr: ' + str(lr))\n",
    "print('l2_coef: ' + str(l2_coef))\n",
    "print('----- Archi. hyperparams -----')\n",
    "print('nb. layers: ' + str(len(hid_units)))\n",
    "print('nb. units per layer: ' + str(hid_units))\n",
    "print('nb. attention heads: ' + str(n_heads))\n",
    "print('residual: ' + str(residual))\n",
    "print('nonlinearity: ' + str(nonlinearity))\n",
    "print('model: ' + str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692361ad",
   "metadata": {},
   "source": [
    "## jhy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c8d4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jhy data\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b08d6c",
   "metadata": {},
   "source": [
    "### sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21589aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112af99c",
   "metadata": {},
   "source": [
    "### load_data_ACM3025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "930d1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "acm_filepath = os.path.abspath(os.path.dirname(os.getcwd())) + '/data/han_data/ACM3025.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9207a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_acm3025(path=acm_filepath):\n",
    "    data = sio.loadmat(path)  # load .mat file\n",
    "    '''\n",
    "    全是ndarray\n",
    "    PTP: (3025, 3025)，全是1\n",
    "    PLP: (3025, 3025)，有0有1，有些向量时相同的\n",
    "    PAP: (3025, 3025)，对角线全是1，其他元素基本是0，很少是1.\n",
    "    feature: (3025, 1870)，由0、1组成。\n",
    "    label: (3025, 3),就3列，1061、965、999\n",
    "    train_idx: (1, 600)，0-2225随机抽取的索引\n",
    "    val_idx: (1, 300)，200-2325之间随机抽取的索引\n",
    "    test_idx: (1, 2125)，300-3024之间随机抽取的索引\n",
    "    '''\n",
    "    truelabels, truefeatures = data['label'], data['feature'].astype(float)\n",
    "    N = truefeatures.shape[0]\n",
    "    rownetworks = [data['PAP'] - np.eye(N), data['PLP'] - np.eye(N)]  # , data['PTP'] - np.eye(N)]\n",
    "    '''\n",
    "    rownetworks: list: 2。第1个元素，ndarray,(3025, 3025)；第2个元素，ndarray，(3025, 3025)\n",
    "    '''\n",
    "    y = truelabels    # shape为(3025, 3)\n",
    "    train_idx = data['train_idx']  # (1, 600)\n",
    "    val_idx = data['val_idx']      # (1, 300)\n",
    "    test_idx = data['test_idx']    # (1, 2125)\n",
    "\n",
    "    train_mask = sample_mask(train_idx, y.shape[0])  # 3025长度的bool list，train_idx位置为True\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])      # 3025长度的boolean list\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])\n",
    "    \n",
    "    # 提取train、val、test的标签\n",
    "    # 所以，为什么不直接用train_test_split呢？\n",
    "    y_train = np.zeros(y.shape)  # shape为(3025, 3)的zero 列表\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y_train[train_mask, :] = y[train_mask, :]  # 取出train_idx为true的label，放入y_train，y_train其余位置为0\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "\n",
    "    # return selected_idx, selected_idx_2\n",
    "    print('y_train:{}, y_val:{}, y_test:{}, train_idx:{}, val_idx:{}, test_idx:{}'.format(y_train.shape,\n",
    "                                                                                          y_val.shape,\n",
    "                                                                                          y_test.shape,\n",
    "                                                                                          train_idx.shape,\n",
    "                                                                                          val_idx.shape,\n",
    "                                                                                          test_idx.shape))\n",
    "    truefeatures_list = [truefeatures, truefeatures, truefeatures]   # truefeatures: (3025, 1870)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dd88a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:(3025, 3), y_val:(3025, 3), y_test:(3025, 3), train_idx:(1, 600), val_idx:(1, 300), test_idx:(1, 2125)\n"
     ]
    }
   ],
   "source": [
    "# use adj_list as fea_list, have a try~\n",
    "adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_acm3025()\n",
    "if featype == 'adj':\n",
    "    fea_list = adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56394450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3025, 1870)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "847f4938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]]),\n",
       " array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]]),\n",
       " array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0444ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3143462f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57b9af",
   "metadata": {},
   "source": [
    "## run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0fa80a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7f3c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "x = x[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e5d3d",
   "metadata": {},
   "source": [
    "### add_data_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8385aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truefeatures: (3025, 1870)\n",
    "nb_nodes = fea_list[0].shape[0]  # 3025\n",
    "ft_size = fea_list[0].shape[1]   # 1870\n",
    "nb_classes = y_train.shape[1]    # 3\n",
    "\n",
    "# adj = adj.todense()\n",
    "\n",
    "# features = features[np.newaxis]  # [1, nb_node, ft_size]\n",
    "fea_list = [fea[np.newaxis] for fea in fea_list]  # np.newaxis行增加一个新的维度\n",
    "'''\n",
    "fea_list:list:3。第一个元素，ndarray, (1, 3025, 1870)\n",
    "'''\n",
    "adj_list = [adj[np.newaxis] for adj in adj_list]  # adj_list: 2. 单个元素(1, 3025, 3025)\n",
    "y_train = y_train[np.newaxis]  # ndarray: (1, 3025, 3)\n",
    "y_val = y_val[np.newaxis]      # ndarray: (1, 3025, 3)\n",
    "y_test = y_test[np.newaxis]    # ndarray: (1, 3025, 3)\n",
    "train_mask = train_mask[np.newaxis]  # ndarray(1, 3025)\n",
    "val_mask = val_mask[np.newaxis]      # ndarray(1, 3025)\n",
    "test_mask = test_mask[np.newaxis]    # ndarray(1, 3025)\n",
    "\n",
    "biases_list = [adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af0066ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3025, 1870)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b402472f",
   "metadata": {},
   "source": [
    "### build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6dec9169",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\3938853343.py:20: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  features_fts = tf.compat.v1.layers.conv1d(features, out_sz, 1, use_bias=False)  # 一维卷积操作, out: (1, 3025, 8)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\3938853343.py:22: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  f_1 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\3938853343.py:23: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  f_2 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\3264048851.py:53: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "Epoch: 0, att_val: [0.5012158  0.49878418]\n",
      "Training: loss = 1.15855, acc = 0.31833 | Val: loss = 1.09910, acc = 0.33333\n",
      "Epoch: 1, att_val: [0.19507875 0.80492055]\n",
      "Training: loss = 1.65728, acc = 0.33333 | Val: loss = 1.09879, acc = 0.33333\n",
      "Epoch: 2, att_val: [0.40036142 0.59963846]\n",
      "Training: loss = 0.82151, acc = 0.65333 | Val: loss = 1.09866, acc = 0.33333\n",
      "Epoch: 3, att_val: [0.5267406  0.47325933]\n",
      "Training: loss = 0.71529, acc = 0.66333 | Val: loss = 1.09866, acc = 0.33333\n",
      "Epoch: 4, att_val: [0.77960074 0.22039962]\n",
      "Training: loss = 0.44379, acc = 0.90167 | Val: loss = 1.09871, acc = 0.33333\n",
      "Epoch: 5, att_val: [0.9308619  0.06913891]\n",
      "Training: loss = 0.30061, acc = 0.96167 | Val: loss = 1.09878, acc = 0.33333\n",
      "Epoch: 6, att_val: [0.9769333  0.02306767]\n",
      "Training: loss = 0.28390, acc = 0.93833 | Val: loss = 1.09887, acc = 0.33333\n",
      "Epoch: 7, att_val: [0.9902196  0.00978026]\n",
      "Training: loss = 0.22362, acc = 0.95500 | Val: loss = 1.09898, acc = 0.33333\n",
      "Epoch: 8, att_val: [0.9946356  0.00536509]\n",
      "Training: loss = 0.17809, acc = 0.96167 | Val: loss = 1.09911, acc = 0.33333\n",
      "Epoch: 9, att_val: [0.9961898 0.0038107]\n",
      "Training: loss = 0.15811, acc = 0.96833 | Val: loss = 1.09925, acc = 0.33333\n",
      "Epoch: 10, att_val: [0.99711096 0.0028864 ]\n",
      "Training: loss = 0.14029, acc = 0.97000 | Val: loss = 1.09940, acc = 0.33333\n",
      "Epoch: 11, att_val: [0.99729943 0.00270031]\n",
      "Training: loss = 0.13553, acc = 0.95833 | Val: loss = 1.09955, acc = 0.33333\n",
      "Epoch: 12, att_val: [0.9975304  0.00246856]\n",
      "Training: loss = 0.12814, acc = 0.96667 | Val: loss = 1.09970, acc = 0.33333\n",
      "Epoch: 13, att_val: [0.99737805 0.00262076]\n",
      "Training: loss = 0.10243, acc = 0.97167 | Val: loss = 1.09984, acc = 0.33333\n",
      "Epoch: 14, att_val: [0.9975256  0.00247301]\n",
      "Training: loss = 0.08937, acc = 0.97833 | Val: loss = 1.09997, acc = 0.33333\n",
      "Epoch: 15, att_val: [0.99730825 0.00269065]\n",
      "Training: loss = 0.09038, acc = 0.97667 | Val: loss = 1.10011, acc = 0.33333\n",
      "Epoch: 16, att_val: [0.99745256 0.00254881]\n",
      "Training: loss = 0.07599, acc = 0.98167 | Val: loss = 1.10024, acc = 0.33333\n",
      "Epoch: 17, att_val: [0.99763465 0.00236626]\n",
      "Training: loss = 0.06848, acc = 0.98167 | Val: loss = 1.10039, acc = 0.33333\n",
      "Epoch: 18, att_val: [0.9971651  0.00283445]\n",
      "Training: loss = 0.06594, acc = 0.98667 | Val: loss = 1.10053, acc = 0.33333\n",
      "Epoch: 19, att_val: [0.99690807 0.00309276]\n",
      "Training: loss = 0.06200, acc = 0.98833 | Val: loss = 1.10070, acc = 0.33333\n",
      "Epoch: 20, att_val: [0.9966562  0.00334468]\n",
      "Training: loss = 0.06640, acc = 0.98500 | Val: loss = 1.10087, acc = 0.33333\n",
      "Epoch: 21, att_val: [0.99608517 0.00391632]\n",
      "Training: loss = 0.05062, acc = 0.99000 | Val: loss = 1.10106, acc = 0.33333\n",
      "Epoch: 22, att_val: [0.9955304  0.00446861]\n",
      "Training: loss = 0.04861, acc = 0.98667 | Val: loss = 1.10120, acc = 0.33333\n",
      "Epoch: 23, att_val: [0.9951888  0.00481122]\n",
      "Training: loss = 0.04731, acc = 0.98500 | Val: loss = 1.10132, acc = 0.33333\n",
      "Epoch: 24, att_val: [0.99436057 0.00563982]\n",
      "Training: loss = 0.04223, acc = 0.99000 | Val: loss = 1.10144, acc = 0.33333\n",
      "Epoch: 25, att_val: [0.99412376 0.00587563]\n",
      "Training: loss = 0.04035, acc = 0.99000 | Val: loss = 1.10154, acc = 0.33333\n",
      "Epoch: 26, att_val: [0.993649   0.00635058]\n",
      "Training: loss = 0.03678, acc = 0.99000 | Val: loss = 1.10160, acc = 0.33333\n",
      "Epoch: 27, att_val: [0.9926853  0.00731404]\n",
      "Training: loss = 0.03928, acc = 0.99167 | Val: loss = 1.10166, acc = 0.33333\n",
      "Epoch: 28, att_val: [0.9938192  0.00618105]\n",
      "Training: loss = 0.03546, acc = 0.99333 | Val: loss = 1.10174, acc = 0.33333\n",
      "Epoch: 29, att_val: [0.992529   0.00747295]\n",
      "Training: loss = 0.03448, acc = 0.98833 | Val: loss = 1.10185, acc = 0.33333\n",
      "Epoch: 30, att_val: [0.99236614 0.00763396]\n",
      "Training: loss = 0.03388, acc = 0.99333 | Val: loss = 1.10198, acc = 0.33333\n",
      "Epoch: 31, att_val: [0.9915706  0.00842941]\n",
      "Training: loss = 0.04141, acc = 0.98833 | Val: loss = 1.10214, acc = 0.33333\n",
      "Epoch: 32, att_val: [0.99003786 0.00996144]\n",
      "Training: loss = 0.03439, acc = 0.99167 | Val: loss = 1.10232, acc = 0.33333\n",
      "Epoch: 33, att_val: [0.9901125  0.00988737]\n",
      "Training: loss = 0.03340, acc = 0.99333 | Val: loss = 1.10250, acc = 0.33333\n",
      "Epoch: 34, att_val: [0.9876995  0.01230075]\n",
      "Training: loss = 0.03517, acc = 0.99000 | Val: loss = 1.10268, acc = 0.33333\n",
      "Epoch: 35, att_val: [0.987266   0.01273506]\n",
      "Training: loss = 0.02980, acc = 0.99333 | Val: loss = 1.10282, acc = 0.33333\n",
      "Epoch: 36, att_val: [0.9866765  0.01332407]\n",
      "Training: loss = 0.03227, acc = 0.99333 | Val: loss = 1.10290, acc = 0.33333\n",
      "Epoch: 37, att_val: [0.9864856  0.01351453]\n",
      "Training: loss = 0.02402, acc = 0.99833 | Val: loss = 1.10294, acc = 0.33333\n",
      "Epoch: 38, att_val: [0.9843048  0.01569414]\n",
      "Training: loss = 0.02804, acc = 0.99167 | Val: loss = 1.10295, acc = 0.33333\n",
      "Epoch: 39, att_val: [0.9849782  0.01502224]\n",
      "Training: loss = 0.02325, acc = 0.99500 | Val: loss = 1.10296, acc = 0.33333\n",
      "Epoch: 40, att_val: [0.98249364 0.01750557]\n",
      "Training: loss = 0.02741, acc = 0.99333 | Val: loss = 1.10294, acc = 0.33333\n",
      "Epoch: 41, att_val: [0.98529136 0.01470981]\n",
      "Training: loss = 0.02812, acc = 0.99167 | Val: loss = 1.10296, acc = 0.33333\n",
      "Epoch: 42, att_val: [0.9858014  0.01419873]\n",
      "Training: loss = 0.02572, acc = 0.99333 | Val: loss = 1.10302, acc = 0.33333\n",
      "Epoch: 43, att_val: [0.9840177  0.01598169]\n",
      "Training: loss = 0.01962, acc = 0.99333 | Val: loss = 1.10311, acc = 0.33333\n",
      "Epoch: 44, att_val: [0.9841118  0.01588734]\n",
      "Training: loss = 0.01869, acc = 0.99667 | Val: loss = 1.10322, acc = 0.33333\n",
      "Epoch: 45, att_val: [0.98279953 0.0172009 ]\n",
      "Training: loss = 0.02105, acc = 0.99667 | Val: loss = 1.10335, acc = 0.33333\n",
      "Epoch: 46, att_val: [0.9818902  0.01811033]\n",
      "Training: loss = 0.01795, acc = 0.99667 | Val: loss = 1.10351, acc = 0.33333\n",
      "Epoch: 47, att_val: [0.9773523  0.02264819]\n",
      "Training: loss = 0.02225, acc = 0.99500 | Val: loss = 1.10367, acc = 0.33333\n",
      "Epoch: 48, att_val: [0.97605646 0.0239439 ]\n",
      "Training: loss = 0.02089, acc = 0.99667 | Val: loss = 1.10384, acc = 0.33333\n",
      "Epoch: 49, att_val: [0.97309095 0.02690941]\n",
      "Training: loss = 0.01839, acc = 0.99667 | Val: loss = 1.10400, acc = 0.33333\n",
      "Epoch: 50, att_val: [0.9681077  0.03189206]\n",
      "Training: loss = 0.01928, acc = 0.99500 | Val: loss = 1.10411, acc = 0.33333\n",
      "Epoch: 51, att_val: [0.9673102  0.03268901]\n",
      "Training: loss = 0.01696, acc = 0.99833 | Val: loss = 1.10422, acc = 0.33333\n",
      "Epoch: 52, att_val: [0.96388686 0.03611348]\n",
      "Training: loss = 0.01923, acc = 0.99833 | Val: loss = 1.10434, acc = 0.33333\n",
      "Epoch: 53, att_val: [0.9599841  0.04001448]\n",
      "Training: loss = 0.02125, acc = 0.99667 | Val: loss = 1.10445, acc = 0.33333\n",
      "Epoch: 54, att_val: [0.9585782  0.04142156]\n",
      "Training: loss = 0.02056, acc = 0.99667 | Val: loss = 1.10456, acc = 0.33333\n",
      "Epoch: 55, att_val: [0.9549242  0.04507729]\n",
      "Training: loss = 0.01913, acc = 0.99667 | Val: loss = 1.10470, acc = 0.33333\n",
      "Epoch: 56, att_val: [0.9517231  0.04827728]\n",
      "Training: loss = 0.01763, acc = 0.99667 | Val: loss = 1.10487, acc = 0.33333\n",
      "Epoch: 57, att_val: [0.938717   0.06128336]\n",
      "Training: loss = 0.01661, acc = 0.99667 | Val: loss = 1.10505, acc = 0.33333\n",
      "Epoch: 58, att_val: [0.9326096 0.067389 ]\n",
      "Training: loss = 0.01892, acc = 0.99500 | Val: loss = 1.10521, acc = 0.33333\n",
      "Epoch: 59, att_val: [0.920406   0.07959329]\n",
      "Training: loss = 0.02196, acc = 0.99500 | Val: loss = 1.10533, acc = 0.33333\n",
      "Epoch: 60, att_val: [0.91373646 0.08626431]\n",
      "Training: loss = 0.01525, acc = 0.99833 | Val: loss = 1.10543, acc = 0.33333\n",
      "Epoch: 61, att_val: [0.9071158  0.09288363]\n",
      "Training: loss = 0.01822, acc = 0.99833 | Val: loss = 1.10552, acc = 0.33333\n",
      "Epoch: 62, att_val: [0.8961488  0.10385189]\n",
      "Training: loss = 0.01652, acc = 0.99667 | Val: loss = 1.10558, acc = 0.33333\n",
      "Epoch: 63, att_val: [0.88981485 0.11018538]\n",
      "Training: loss = 0.01649, acc = 0.99833 | Val: loss = 1.10563, acc = 0.33333\n",
      "Epoch: 64, att_val: [0.88197273 0.11802951]\n",
      "Training: loss = 0.01542, acc = 0.99833 | Val: loss = 1.10565, acc = 0.33333\n",
      "Epoch: 65, att_val: [0.8819957  0.11800335]\n",
      "Training: loss = 0.01475, acc = 0.99833 | Val: loss = 1.10563, acc = 0.33333\n",
      "Epoch: 66, att_val: [0.87931585 0.12068339]\n",
      "Training: loss = 0.01811, acc = 0.99833 | Val: loss = 1.10562, acc = 0.33333\n",
      "Epoch: 67, att_val: [0.885212   0.11478656]\n",
      "Training: loss = 0.01277, acc = 1.00000 | Val: loss = 1.10564, acc = 0.33333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, att_val: [0.8820521  0.11794862]\n",
      "Training: loss = 0.01197, acc = 0.99833 | Val: loss = 1.10569, acc = 0.33333\n",
      "Epoch: 69, att_val: [0.8754525  0.12454744]\n",
      "Training: loss = 0.01206, acc = 0.99833 | Val: loss = 1.10572, acc = 0.33333\n",
      "Epoch: 70, att_val: [0.8736175  0.12638247]\n",
      "Training: loss = 0.01329, acc = 0.99833 | Val: loss = 1.10578, acc = 0.33333\n",
      "Epoch: 71, att_val: [0.86625266 0.1337487 ]\n",
      "Training: loss = 0.02476, acc = 0.99000 | Val: loss = 1.10590, acc = 0.33333\n",
      "Epoch: 72, att_val: [0.83868945 0.16130945]\n",
      "Training: loss = 0.01401, acc = 0.99833 | Val: loss = 1.10597, acc = 0.33333\n",
      "Epoch: 73, att_val: [0.81534326 0.18465658]\n",
      "Training: loss = 0.01649, acc = 0.99833 | Val: loss = 1.10594, acc = 0.33333\n",
      "Epoch: 74, att_val: [0.8200859  0.17991322]\n",
      "Training: loss = 0.01760, acc = 0.99500 | Val: loss = 1.10583, acc = 0.33333\n",
      "Epoch: 75, att_val: [0.8485408  0.15145984]\n",
      "Training: loss = 0.01429, acc = 0.99667 | Val: loss = 1.10577, acc = 0.33333\n",
      "Epoch: 76, att_val: [0.85703003 0.14297174]\n",
      "Training: loss = 0.02145, acc = 0.99167 | Val: loss = 1.10573, acc = 0.33333\n",
      "Epoch: 77, att_val: [0.8648895  0.13510999]\n",
      "Training: loss = 0.01491, acc = 0.99833 | Val: loss = 1.10575, acc = 0.33333\n",
      "Epoch: 78, att_val: [0.8520412  0.14795826]\n",
      "Training: loss = 0.01340, acc = 1.00000 | Val: loss = 1.10579, acc = 0.33333\n",
      "Epoch: 79, att_val: [0.8504911  0.14950944]\n",
      "Training: loss = 0.01139, acc = 0.99833 | Val: loss = 1.10587, acc = 0.33333\n",
      "Epoch: 80, att_val: [0.83351153 0.16648825]\n",
      "Training: loss = 0.01093, acc = 0.99667 | Val: loss = 1.10595, acc = 0.33333\n",
      "Epoch: 81, att_val: [0.8165583  0.18344297]\n",
      "Training: loss = 0.01582, acc = 0.99667 | Val: loss = 1.10592, acc = 0.33333\n",
      "Epoch: 82, att_val: [0.8336655  0.16633354]\n",
      "Training: loss = 0.01207, acc = 1.00000 | Val: loss = 1.10593, acc = 0.33333\n",
      "Epoch: 83, att_val: [0.8394132  0.16058657]\n",
      "Training: loss = 0.01138, acc = 0.99833 | Val: loss = 1.10594, acc = 0.33333\n",
      "Epoch: 84, att_val: [0.8454945  0.15450549]\n",
      "Training: loss = 0.01825, acc = 0.99667 | Val: loss = 1.10593, acc = 0.33333\n",
      "Epoch: 85, att_val: [0.8499444 0.1500558]\n",
      "Training: loss = 0.01164, acc = 0.99833 | Val: loss = 1.10597, acc = 0.33333\n",
      "Epoch: 86, att_val: [0.842525   0.15747483]\n",
      "Training: loss = 0.01264, acc = 0.99667 | Val: loss = 1.10607, acc = 0.33333\n",
      "Epoch: 87, att_val: [0.82609797 0.17390145]\n",
      "Training: loss = 0.00910, acc = 1.00000 | Val: loss = 1.10616, acc = 0.33333\n",
      "Epoch: 88, att_val: [0.7938726  0.20612814]\n",
      "Training: loss = 0.01131, acc = 0.99667 | Val: loss = 1.10614, acc = 0.33333\n",
      "Epoch: 89, att_val: [0.79639304 0.2036064 ]\n",
      "Training: loss = 0.01410, acc = 0.99667 | Val: loss = 1.10608, acc = 0.33333\n",
      "Epoch: 90, att_val: [0.80320716 0.19679332]\n",
      "Training: loss = 0.01213, acc = 0.99833 | Val: loss = 1.10600, acc = 0.33333\n",
      "Epoch: 91, att_val: [0.81973535 0.18026464]\n",
      "Training: loss = 0.01805, acc = 0.99833 | Val: loss = 1.10595, acc = 0.33333\n",
      "Epoch: 92, att_val: [0.8215756  0.17842512]\n",
      "Training: loss = 0.00965, acc = 0.99833 | Val: loss = 1.10601, acc = 0.33333\n",
      "Epoch: 93, att_val: [0.7857296  0.21427107]\n",
      "Training: loss = 0.01507, acc = 0.99833 | Val: loss = 1.10600, acc = 0.33333\n",
      "Epoch: 94, att_val: [0.77457154 0.22542848]\n",
      "Training: loss = 0.01137, acc = 1.00000 | Val: loss = 1.10596, acc = 0.33333\n",
      "Epoch: 95, att_val: [0.7783114 0.2216893]\n",
      "Training: loss = 0.01109, acc = 0.99833 | Val: loss = 1.10584, acc = 0.33333\n",
      "Epoch: 96, att_val: [0.8126787 0.1873222]\n",
      "Training: loss = 0.01227, acc = 1.00000 | Val: loss = 1.10578, acc = 0.33333\n",
      "Epoch: 97, att_val: [0.8257435  0.17425567]\n",
      "Training: loss = 0.01264, acc = 0.99667 | Val: loss = 1.10586, acc = 0.33333\n",
      "Epoch: 98, att_val: [0.8104271  0.18957298]\n",
      "Training: loss = 0.00952, acc = 1.00000 | Val: loss = 1.10594, acc = 0.33333\n",
      "Epoch: 99, att_val: [0.78818303 0.21181838]\n",
      "Training: loss = 0.00891, acc = 1.00000 | Val: loss = 1.10602, acc = 0.33333\n",
      "Epoch: 100, att_val: [0.7653203  0.23468056]\n",
      "Training: loss = 0.00924, acc = 1.00000 | Val: loss = 1.10605, acc = 0.33333\n",
      "Epoch: 101, att_val: [0.7581595 0.2418397]\n",
      "Training: loss = 0.01768, acc = 0.99500 | Val: loss = 1.10591, acc = 0.33333\n",
      "Epoch: 102, att_val: [0.80619675 0.19380252]\n",
      "Training: loss = 0.00858, acc = 0.99833 | Val: loss = 1.10582, acc = 0.33333\n",
      "Epoch: 103, att_val: [0.8255324  0.17446755]\n",
      "Training: loss = 0.02562, acc = 0.99500 | Val: loss = 1.10585, acc = 0.33333\n",
      "Epoch: 104, att_val: [0.813765  0.1862353]\n",
      "Training: loss = 0.01117, acc = 0.99833 | Val: loss = 1.10594, acc = 0.33333\n",
      "Epoch: 105, att_val: [0.77376163 0.22623757]\n",
      "Training: loss = 0.01038, acc = 0.99833 | Val: loss = 1.10600, acc = 0.33333\n",
      "Epoch: 106, att_val: [0.7578847  0.24211499]\n",
      "Training: loss = 0.01050, acc = 0.99833 | Val: loss = 1.10596, acc = 0.33333\n",
      "Epoch: 107, att_val: [0.75871193 0.2412876 ]\n",
      "Training: loss = 0.01796, acc = 0.99500 | Val: loss = 1.10572, acc = 0.33333\n",
      "Epoch: 108, att_val: [0.82294554 0.17705615]\n",
      "Training: loss = 0.01158, acc = 0.99833 | Val: loss = 1.10560, acc = 0.33333\n",
      "Epoch: 109, att_val: [0.85425055 0.14574929]\n",
      "Training: loss = 0.02087, acc = 0.99667 | Val: loss = 1.10569, acc = 0.33333\n",
      "Epoch: 110, att_val: [0.8290906  0.17090957]\n",
      "Training: loss = 0.01061, acc = 1.00000 | Val: loss = 1.10582, acc = 0.33333\n",
      "Epoch: 111, att_val: [0.80002165 0.1999771 ]\n",
      "Training: loss = 0.00606, acc = 1.00000 | Val: loss = 1.10595, acc = 0.33333\n",
      "Epoch: 112, att_val: [0.7638944 0.2361049]\n",
      "Training: loss = 0.01586, acc = 0.99333 | Val: loss = 1.10588, acc = 0.33333\n",
      "Epoch: 113, att_val: [0.7853774 0.2146233]\n",
      "Training: loss = 0.00879, acc = 1.00000 | Val: loss = 1.10577, acc = 0.33333\n",
      "Epoch: 114, att_val: [0.7998401  0.20016053]\n",
      "Training: loss = 0.01105, acc = 0.99833 | Val: loss = 1.10568, acc = 0.33333\n",
      "Epoch: 115, att_val: [0.820855   0.17914495]\n",
      "Training: loss = 0.01171, acc = 1.00000 | Val: loss = 1.10572, acc = 0.33333\n",
      "Epoch: 116, att_val: [0.8144549 0.1855454]\n",
      "Training: loss = 0.00779, acc = 1.00000 | Val: loss = 1.10577, acc = 0.33333\n",
      "Epoch: 117, att_val: [0.80175126 0.19824867]\n",
      "Training: loss = 0.01078, acc = 0.99833 | Val: loss = 1.10580, acc = 0.33333\n",
      "Epoch: 118, att_val: [0.7901928 0.2098082]\n",
      "Training: loss = 0.00707, acc = 1.00000 | Val: loss = 1.10579, acc = 0.33333\n",
      "Epoch: 119, att_val: [0.79834986 0.2016508 ]\n",
      "Training: loss = 0.00930, acc = 1.00000 | Val: loss = 1.10584, acc = 0.33333\n",
      "Epoch: 120, att_val: [0.77109075 0.22890931]\n",
      "Training: loss = 0.00751, acc = 1.00000 | Val: loss = 1.10583, acc = 0.33333\n",
      "Epoch: 121, att_val: [0.7694359  0.23056428]\n",
      "Training: loss = 0.01051, acc = 0.99667 | Val: loss = 1.10574, acc = 0.33333\n",
      "Epoch: 122, att_val: [0.7703126 0.2296877]\n",
      "Training: loss = 0.00808, acc = 1.00000 | Val: loss = 1.10556, acc = 0.33333\n",
      "Epoch: 123, att_val: [0.79241    0.20758952]\n",
      "Training: loss = 0.01125, acc = 0.99667 | Val: loss = 1.10528, acc = 0.33333\n",
      "Epoch: 124, att_val: [0.850205   0.14979433]\n",
      "Training: loss = 0.02298, acc = 0.99500 | Val: loss = 1.10529, acc = 0.33333\n",
      "Epoch: 125, att_val: [0.84410244 0.15589789]\n",
      "Training: loss = 0.01051, acc = 1.00000 | Val: loss = 1.10551, acc = 0.33333\n",
      "Epoch: 126, att_val: [0.77054125 0.22945952]\n",
      "Training: loss = 0.01083, acc = 0.99667 | Val: loss = 1.10560, acc = 0.33333\n",
      "Epoch: 127, att_val: [0.7306258 0.2693744]\n",
      "Training: loss = 0.01418, acc = 0.99667 | Val: loss = 1.10537, acc = 0.33333\n",
      "Epoch: 128, att_val: [0.8020596  0.19794044]\n",
      "Training: loss = 0.00752, acc = 1.00000 | Val: loss = 1.10524, acc = 0.33333\n",
      "Epoch: 129, att_val: [0.82857597 0.17142291]\n",
      "Training: loss = 0.01287, acc = 0.99667 | Val: loss = 1.10523, acc = 0.33333\n",
      "Epoch: 130, att_val: [0.8255902  0.17441013]\n",
      "Training: loss = 0.00930, acc = 1.00000 | Val: loss = 1.10527, acc = 0.33333\n",
      "Epoch: 131, att_val: [0.8131075  0.18689308]\n",
      "Training: loss = 0.00983, acc = 0.99833 | Val: loss = 1.10534, acc = 0.33333\n",
      "Epoch: 132, att_val: [0.7844827  0.21551792]\n",
      "Training: loss = 0.00817, acc = 1.00000 | Val: loss = 1.10535, acc = 0.33333\n",
      "Epoch: 133, att_val: [0.77400875 0.22599067]\n",
      "Training: loss = 0.00863, acc = 0.99833 | Val: loss = 1.10525, acc = 0.33333\n",
      "Epoch: 134, att_val: [0.7891428  0.21085732]\n",
      "Training: loss = 0.00870, acc = 0.99833 | Val: loss = 1.10512, acc = 0.33333\n",
      "Epoch: 135, att_val: [0.81907886 0.1809213 ]\n",
      "Training: loss = 0.00605, acc = 1.00000 | Val: loss = 1.10504, acc = 0.33333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136, att_val: [0.8308     0.16920099]\n",
      "Training: loss = 0.01317, acc = 0.99833 | Val: loss = 1.10509, acc = 0.33333\n",
      "Epoch: 137, att_val: [0.81146276 0.18853879]\n",
      "Training: loss = 0.00557, acc = 1.00000 | Val: loss = 1.10518, acc = 0.33333\n",
      "Epoch: 138, att_val: [0.7738308  0.22616969]\n",
      "Training: loss = 0.00643, acc = 1.00000 | Val: loss = 1.10525, acc = 0.33333\n",
      "Epoch: 139, att_val: [0.7437889  0.25620997]\n",
      "Training: loss = 0.01913, acc = 0.99500 | Val: loss = 1.10497, acc = 0.33333\n",
      "Epoch: 140, att_val: [0.8117277  0.18827158]\n",
      "Training: loss = 0.00992, acc = 0.99833 | Val: loss = 1.10473, acc = 0.33333\n",
      "Epoch: 141, att_val: [0.8594375  0.14056261]\n",
      "Training: loss = 0.01753, acc = 0.99833 | Val: loss = 1.10495, acc = 0.33333\n",
      "Epoch: 142, att_val: [0.82083    0.17917019]\n",
      "Training: loss = 0.01355, acc = 0.99667 | Val: loss = 1.10525, acc = 0.33333\n",
      "Epoch: 143, att_val: [0.74386966 0.2561308 ]\n",
      "Training: loss = 0.00824, acc = 1.00000 | Val: loss = 1.10539, acc = 0.33333\n",
      "Epoch: 144, att_val: [0.69159025 0.308409  ]\n",
      "Training: loss = 0.01912, acc = 0.99167 | Val: loss = 1.10509, acc = 0.33333\n",
      "Epoch: 145, att_val: [0.79024756 0.20975316]\n",
      "Training: loss = 0.00793, acc = 1.00000 | Val: loss = 1.10482, acc = 0.33333\n",
      "Epoch: 146, att_val: [0.8543877  0.14561246]\n",
      "Training: loss = 0.01463, acc = 0.99500 | Val: loss = 1.10483, acc = 0.33333\n",
      "Epoch: 147, att_val: [0.8580846  0.14191496]\n",
      "Training: loss = 0.01769, acc = 0.99333 | Val: loss = 1.10511, acc = 0.33333\n",
      "Epoch: 148, att_val: [0.824638   0.17536217]\n",
      "Training: loss = 0.01231, acc = 0.99667 | Val: loss = 1.10550, acc = 0.33333\n",
      "Epoch: 149, att_val: [0.7625852  0.23741524]\n",
      "Training: loss = 0.00644, acc = 0.99833 | Val: loss = 1.10580, acc = 0.33333\n",
      "Epoch: 150, att_val: [0.7142285 0.2857721]\n",
      "Training: loss = 0.02077, acc = 0.99667 | Val: loss = 1.10585, acc = 0.33333\n",
      "Epoch: 151, att_val: [0.7019141  0.29808632]\n",
      "Training: loss = 0.02712, acc = 0.99333 | Val: loss = 1.10553, acc = 0.33333\n",
      "Epoch: 152, att_val: [0.77452034 0.225478  ]\n",
      "Training: loss = 0.00923, acc = 0.99833 | Val: loss = 1.10514, acc = 0.33333\n",
      "Epoch: 153, att_val: [0.8376107  0.16238886]\n",
      "Training: loss = 0.01138, acc = 0.99833 | Val: loss = 1.10486, acc = 0.33333\n",
      "Epoch: 154, att_val: [0.8831593  0.11684085]\n",
      "Training: loss = 0.02837, acc = 0.99000 | Val: loss = 1.10498, acc = 0.33333\n",
      "Epoch: 155, att_val: [0.8723797  0.12761916]\n",
      "Training: loss = 0.01508, acc = 0.99833 | Val: loss = 1.10534, acc = 0.33333\n",
      "Epoch: 156, att_val: [0.8345148 0.1654857]\n",
      "Training: loss = 0.00967, acc = 1.00000 | Val: loss = 1.10570, acc = 0.33333\n",
      "Epoch: 157, att_val: [0.79705083 0.20294958]\n",
      "Training: loss = 0.00981, acc = 1.00000 | Val: loss = 1.10592, acc = 0.33333\n",
      "Epoch: 158, att_val: [0.76414603 0.23585387]\n",
      "Training: loss = 0.01091, acc = 0.99833 | Val: loss = 1.10596, acc = 0.33333\n",
      "Epoch: 159, att_val: [0.7637311 0.2362691]\n",
      "Training: loss = 0.02807, acc = 0.99000 | Val: loss = 1.10559, acc = 0.33333\n",
      "Epoch: 160, att_val: [0.83898133 0.16101737]\n",
      "Training: loss = 0.01249, acc = 0.99833 | Val: loss = 1.10532, acc = 0.33333\n",
      "Epoch: 161, att_val: [0.8813517  0.11864887]\n",
      "Training: loss = 0.02879, acc = 0.98667 | Val: loss = 1.10549, acc = 0.33333\n",
      "Epoch: 162, att_val: [0.8678276  0.13217402]\n",
      "Training: loss = 0.01014, acc = 0.99833 | Val: loss = 1.10579, acc = 0.33333\n",
      "Epoch: 163, att_val: [0.83720195 0.16279766]\n",
      "Training: loss = 0.01395, acc = 0.99500 | Val: loss = 1.10609, acc = 0.33333\n",
      "Epoch: 164, att_val: [0.7902194  0.20978062]\n",
      "Training: loss = 0.00710, acc = 1.00000 | Val: loss = 1.10632, acc = 0.33333\n",
      "Epoch: 165, att_val: [0.75951695 0.24048193]\n",
      "Training: loss = 0.00956, acc = 0.99667 | Val: loss = 1.10641, acc = 0.33333\n",
      "Epoch: 166, att_val: [0.73643273 0.26356712]\n",
      "Training: loss = 0.02113, acc = 0.99167 | Val: loss = 1.10625, acc = 0.33333\n",
      "Epoch: 167, att_val: [0.77448016 0.22551984]\n",
      "Training: loss = 0.00817, acc = 1.00000 | Val: loss = 1.10603, acc = 0.33333\n",
      "Epoch: 168, att_val: [0.80135685 0.19864275]\n",
      "Training: loss = 0.00683, acc = 1.00000 | Val: loss = 1.10584, acc = 0.33333\n",
      "Epoch: 169, att_val: [0.82933927 0.17065944]\n",
      "Training: loss = 0.00903, acc = 0.99833 | Val: loss = 1.10574, acc = 0.33333\n",
      "Epoch: 170, att_val: [0.83789057 0.16210832]\n",
      "Training: loss = 0.00853, acc = 0.99833 | Val: loss = 1.10576, acc = 0.33333\n",
      "Epoch: 171, att_val: [0.8296564  0.17034462]\n",
      "Training: loss = 0.00896, acc = 0.99833 | Val: loss = 1.10581, acc = 0.33333\n",
      "Epoch: 172, att_val: [0.8139463  0.18605347]\n",
      "Training: loss = 0.01111, acc = 0.99833 | Val: loss = 1.10594, acc = 0.33333\n",
      "Epoch: 173, att_val: [0.78328276 0.21671765]\n",
      "Training: loss = 0.00694, acc = 1.00000 | Val: loss = 1.10605, acc = 0.33333\n",
      "Epoch: 174, att_val: [0.74522704 0.25477308]\n",
      "Training: loss = 0.01016, acc = 0.99833 | Val: loss = 1.10598, acc = 0.33333\n",
      "Epoch: 175, att_val: [0.73447144 0.2655273 ]\n",
      "Training: loss = 0.01565, acc = 0.99500 | Val: loss = 1.10564, acc = 0.33333\n",
      "Epoch: 176, att_val: [0.7716415  0.22835867]\n",
      "Training: loss = 0.00773, acc = 1.00000 | Val: loss = 1.10525, acc = 0.33333\n",
      "Epoch: 177, att_val: [0.8183386  0.18166053]\n",
      "Training: loss = 0.00956, acc = 0.99833 | Val: loss = 1.10486, acc = 0.33333\n",
      "Epoch: 178, att_val: [0.8511014  0.14889847]\n",
      "Training: loss = 0.01749, acc = 0.99333 | Val: loss = 1.10478, acc = 0.33333\n",
      "Epoch: 179, att_val: [0.8426993  0.15730228]\n",
      "Training: loss = 0.01163, acc = 0.99833 | Val: loss = 1.10477, acc = 0.33333\n",
      "Epoch: 180, att_val: [0.8333504  0.16664864]\n",
      "Training: loss = 0.01121, acc = 0.99667 | Val: loss = 1.10493, acc = 0.33333\n",
      "Epoch: 181, att_val: [0.7974967  0.20250523]\n",
      "Training: loss = 0.00737, acc = 1.00000 | Val: loss = 1.10506, acc = 0.33333\n",
      "Epoch: 182, att_val: [0.7560174 0.2439823]\n",
      "Training: loss = 0.01185, acc = 0.99833 | Val: loss = 1.10503, acc = 0.33333\n",
      "Epoch: 183, att_val: [0.7644805  0.23552106]\n",
      "Training: loss = 0.00886, acc = 1.00000 | Val: loss = 1.10483, acc = 0.33333\n",
      "Epoch: 184, att_val: [0.80096686 0.19903423]\n",
      "Training: loss = 0.00955, acc = 0.99833 | Val: loss = 1.10468, acc = 0.33333\n",
      "Epoch: 185, att_val: [0.8132905 0.1867106]\n",
      "Training: loss = 0.00710, acc = 1.00000 | Val: loss = 1.10456, acc = 0.33333\n",
      "Epoch: 186, att_val: [0.8270853  0.17291442]\n",
      "Training: loss = 0.00644, acc = 0.99833 | Val: loss = 1.10456, acc = 0.33333\n",
      "Epoch: 187, att_val: [0.8211493  0.17885102]\n",
      "Training: loss = 0.00771, acc = 1.00000 | Val: loss = 1.10468, acc = 0.33333\n",
      "Epoch: 188, att_val: [0.80099607 0.19900407]\n",
      "Training: loss = 0.00580, acc = 1.00000 | Val: loss = 1.10481, acc = 0.33333\n",
      "Epoch: 189, att_val: [0.77485335 0.225146  ]\n",
      "Training: loss = 0.01158, acc = 0.99667 | Val: loss = 1.10483, acc = 0.33333\n",
      "Epoch: 190, att_val: [0.769454   0.23054601]\n",
      "Training: loss = 0.00764, acc = 0.99833 | Val: loss = 1.10477, acc = 0.33333\n",
      "Epoch: 191, att_val: [0.7620121  0.23798823]\n",
      "Training: loss = 0.00704, acc = 0.99833 | Val: loss = 1.10463, acc = 0.33333\n",
      "Epoch: 192, att_val: [0.7751357 0.2248635]\n",
      "Training: loss = 0.00821, acc = 0.99667 | Val: loss = 1.10451, acc = 0.33333\n",
      "Epoch: 193, att_val: [0.78844595 0.21155435]\n",
      "Training: loss = 0.01053, acc = 0.99833 | Val: loss = 1.10440, acc = 0.33333\n",
      "Epoch: 194, att_val: [0.79351175 0.20648699]\n",
      "Training: loss = 0.01096, acc = 0.99833 | Val: loss = 1.10441, acc = 0.33333\n",
      "Epoch: 195, att_val: [0.7830375  0.21696301]\n",
      "Training: loss = 0.01056, acc = 0.99667 | Val: loss = 1.10447, acc = 0.33333\n",
      "Epoch: 196, att_val: [0.75874054 0.24126057]\n",
      "Training: loss = 0.00851, acc = 0.99667 | Val: loss = 1.10444, acc = 0.33333\n",
      "Epoch: 197, att_val: [0.7490606 0.2509391]\n",
      "Training: loss = 0.00857, acc = 0.99833 | Val: loss = 1.10426, acc = 0.33333\n",
      "Epoch: 198, att_val: [0.7679189  0.23208058]\n",
      "Training: loss = 0.00433, acc = 1.00000 | Val: loss = 1.10413, acc = 0.33333\n",
      "Epoch: 199, att_val: [0.7716346  0.22836535]\n",
      "Training: loss = 0.00507, acc = 1.00000 | Val: loss = 1.10401, acc = 0.33333\n",
      "INFO:tensorflow:Restoring parameters from D:\\PycharmProjects\\GNN Algorithms/data/han_data/acm/acm_allMP_multi_fea_.ckpt\n",
      "load model from : D:\\PycharmProjects\\GNN Algorithms/data/han_data/acm/acm_allMP_multi_fea_.ckpt\n",
      "Test loss: 1.0991134643554688 ; Test accuracy: 0.3129412531852722\n",
      "start knn, kmean.....\n",
      "xx: (2125, 64), yy: (2125, 3)\n",
      "KNN(10avg, split:0.2, k=5) f1_macro: 0.1673, f1_micro: 0.3353\n",
      "KNN(10avg, split:0.4, k=5) f1_macro: 0.1650, f1_micro: 0.3291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN(10avg, split:0.6, k=5) f1_macro: 0.1670, f1_micro: 0.3344\n",
      "KNN(10avg, split:0.8, k=5) f1_macro: 0.1720, f1_micro: 0.3480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMI (10 avg): 0.0000 , ARI (10avg): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_9488\\743933309.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  estimator.fit(x, y)\n"
     ]
    }
   ],
   "source": [
    "print('build graph...')\n",
    "with tf.Graph().as_default():  # 创建一个新的计算图\n",
    "    with tf.name_scope('input'):  # 创建一个上下文管理器\n",
    "        ftr_in_list = [tf.compat.v1.placeholder(dtype=tf.float32,  # 占位符，提前分配必要的内存\n",
    "                                      shape=(batch_size, nb_nodes, ft_size),  # batch_size:1, nb_nodes:3025, fea_size: 1870\n",
    "                                      name='ftr_in_{}'.format(i))\n",
    "                       for i in range(len(fea_list))]  # fea_list,长度为3，内部单个元素，(1, 3025, 1870)\n",
    "        bias_in_list = [tf.compat.v1.placeholder(dtype=tf.float32,\n",
    "                                       shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                       name='bias_in_{}'.format(i))\n",
    "                        for i in range(len(biases_list))]  # 邻接矩阵转换成的biases_list: 2. 单个元素占位符tensor, (1, 3025, 3025)\n",
    "        lbl_in = tf.compat.v1.placeholder(dtype=tf.int32, \n",
    "                                        shape=(batch_size, nb_nodes, nb_classes),   # tensor, nb_classes: 3\n",
    "                                        name='lbl_in')   \n",
    "        msk_in = tf.compat.v1.placeholder(dtype=tf.int32, \n",
    "                                        shape=(batch_size, nb_nodes),  # tensor, (1, 3025)\n",
    "                                        name='msk_in')\n",
    "        attn_drop = tf.compat.v1.placeholder(dtype=tf.float32, shape=(), name='attn_drop')  # tensor, ()\n",
    "        ffd_drop = tf.compat.v1.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')  # # tensor, ()\n",
    "        is_train = tf.compat.v1.placeholder(dtype=tf.bool, shape=(), name='is_train')  # # tensor, ()\n",
    "    # forward；model = HeteGAT_multi\n",
    "    logits, final_embedding, att_val = model.inference(ftr_in_list,  # list:3, tensor（1， 3025， 1870）\n",
    "                                                       nb_classes,   # 3\n",
    "                                                       nb_nodes,     # 3025\n",
    "                                                       is_train,     # bool\n",
    "                                                       attn_drop,    # tensor, ()\n",
    "                                                       ffd_drop,     # tensor, ()\n",
    "                                                       bias_mat_list=bias_in_list,  # list:2, tensor(1, 3025, 3025)\n",
    "                                                       hid_units=hid_units,   # hid_units:[8]\n",
    "                                                       n_heads=n_heads,       # n_heads: [8, 1]\n",
    "                                                       residual=residual,     # residual: False\n",
    "                                                       activation=nonlinearity)  # nonlinearity:tf.nn.elu\n",
    "\n",
    "    # cal masked_loss\n",
    "    log_resh = tf.reshape(logits, [-1, nb_classes])  # （3025， 3）\n",
    "    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])  # （3025， 3）\n",
    "    msk_resh = tf.reshape(msk_in, [-1])              # mask，（3025， ）\n",
    "    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)  # 占位符计算softmax cross_entropy based on (pred, y)\n",
    "    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)           # 计算accuracy\n",
    "    # optimzie\n",
    "    train_op = model.training(loss, lr, l2_coef)  # lr = 0.005、l2_coef = 0.001\n",
    "\n",
    "    saver = tf.compat.v1.train.Saver()  # 用于保存模型\n",
    "\n",
    "    init_op = tf.group(tf.compat.v1.global_variables_initializer(),  # 全局变量初始化；group组合多个operation\n",
    "                       tf.compat.v1.local_variables_initializer())\n",
    "\n",
    "    vlss_mn = np.inf\n",
    "    vacc_mx = 0.0\n",
    "    curr_step = 0\n",
    "\n",
    "    with tf.compat.v1.Session(config=config) as sess:  # 创建session\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss_avg = 0\n",
    "        train_acc_avg = 0\n",
    "        val_loss_avg = 0\n",
    "        val_acc_avg = 0\n",
    "\n",
    "        for epoch in range(nb_epochs):  # 200\n",
    "            tr_step = 0\n",
    "           \n",
    "            tr_size = fea_list[0].shape[0]\n",
    "            # ================   training    ============\n",
    "            while tr_step * batch_size < tr_size:\n",
    "                # feature,占位符内存已经分配完毕，fea_list是真实数据，输入进行训练模型\n",
    "                fd1 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]  # dict:3. 每个元素tensor, (1, 3025, 1870)\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                # bias\n",
    "                fd2 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]  # dict: 2. 每个元素tensor, (1, 3025, 3025)\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                # other params\n",
    "                fd3 = {lbl_in: y_train[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       msk_in: train_mask[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       is_train: True,\n",
    "                       attn_drop: 0.6,\n",
    "                       ffd_drop: 0.6}\n",
    "                fd = fd1\n",
    "                fd.update(fd2)  # 字典update方法\n",
    "                fd.update(fd3)  # 获得字典形式的所有数据、参数\n",
    "                # training操作：更新权重；计算loss；计算accuracy；attention概率\n",
    "                _, loss_value_tr, acc_tr, att_val_train = sess.run([train_op, loss, accuracy, att_val],\n",
    "                                                                   feed_dict=fd)\n",
    "                train_loss_avg += loss_value_tr\n",
    "                train_acc_avg += acc_tr\n",
    "                tr_step += 1\n",
    "\n",
    "            vl_step = 0\n",
    "            vl_size = fea_list[0].shape[0]\n",
    "            # =============   val       =================\n",
    "            while vl_step * batch_size < vl_size:\n",
    "                # fd1 = {ftr_in: features[vl_step * batch_size:(vl_step + 1) * batch_size]}\n",
    "                fd1 = {i: d[vl_step * batch_size:(vl_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[vl_step * batch_size:(vl_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_val[vl_step * batch_size:(vl_step + 1) * batch_size],\n",
    "                       msk_in: val_mask[vl_step * batch_size:(vl_step + 1) * batch_size],\n",
    "                       is_train: False,\n",
    "                       attn_drop: 0.0,\n",
    "                       ffd_drop: 0.0}\n",
    "          \n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                loss_value_vl, acc_vl = sess.run([loss, accuracy],\n",
    "                                                 feed_dict=fd)\n",
    "                val_loss_avg += loss_value_vl\n",
    "                val_acc_avg += acc_vl\n",
    "                vl_step += 1\n",
    "            # import pdb; pdb.set_trace()\n",
    "            print('Epoch: {}, att_val: {}'.format(epoch, np.mean(att_val_train, axis=0)))\n",
    "            print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n",
    "                  (train_loss_avg / tr_step, train_acc_avg / tr_step,\n",
    "                   val_loss_avg / vl_step, val_acc_avg / vl_step))\n",
    "            \n",
    "            # =============   judging  =================\n",
    "            if val_acc_avg / vl_step >= vacc_mx or val_loss_avg / vl_step <= vlss_mn:\n",
    "                if val_acc_avg / vl_step >= vacc_mx and val_loss_avg / vl_step <= vlss_mn:\n",
    "                    vacc_early_model = val_acc_avg / vl_step\n",
    "                    vlss_early_model = val_loss_avg / vl_step\n",
    "                    saver.save(sess, checkpt_file)\n",
    "                vacc_mx = np.max((val_acc_avg / vl_step, vacc_mx))\n",
    "                vlss_mn = np.min((val_loss_avg / vl_step, vlss_mn))\n",
    "                curr_step = 0\n",
    "            else:\n",
    "                curr_step += 1\n",
    "                if curr_step == patience:\n",
    "                    print('Early stop! Min loss: ', vlss_mn,\n",
    "                          ', Max accuracy: ', vacc_mx)\n",
    "                    print('Early stop model validation loss: ',\n",
    "                          vlss_early_model, ', accuracy: ', vacc_early_model)\n",
    "                    break\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "        \n",
    "        # loading model params\n",
    "        saver.restore(sess, checkpt_file)\n",
    "        print('load model from : {}'.format(checkpt_file))\n",
    "        ts_size = fea_list[0].shape[0]\n",
    "        ts_step = 0\n",
    "        ts_loss = 0.0\n",
    "        ts_acc = 0.0\n",
    "        \n",
    "        # ============= testing =================\n",
    "        while ts_step * batch_size < ts_size:\n",
    "            # fd1 = {ftr_in: features[ts_step * batch_size:(ts_step + 1) * batch_size]}\n",
    "            fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(ftr_in_list, fea_list)}\n",
    "            fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(bias_in_list, biases_list)}\n",
    "            fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                   msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "            \n",
    "                   is_train: False,\n",
    "                   attn_drop: 0.0,\n",
    "                   ffd_drop: 0.0}\n",
    "        \n",
    "            fd = fd1\n",
    "            fd.update(fd2)\n",
    "            fd.update(fd3)\n",
    "            loss_value_ts, acc_ts, jhy_final_embedding = sess.run([loss, accuracy, final_embedding],\n",
    "                                                                  feed_dict=fd)\n",
    "            ts_loss += loss_value_ts\n",
    "            ts_acc += acc_ts\n",
    "            ts_step += 1\n",
    "\n",
    "        print('Test loss:', ts_loss / ts_step,\n",
    "              '; Test accuracy:', ts_acc / ts_step)\n",
    "\n",
    "        print('start knn, kmean.....')\n",
    "        xx = np.expand_dims(jhy_final_embedding, axis=0)[test_mask]\n",
    "  \n",
    "        from numpy import linalg as LA\n",
    "\n",
    "        # xx = xx / LA.norm(xx, axis=1)\n",
    "        yy = y_test[test_mask]\n",
    "\n",
    "        print('xx: {}, yy: {}'.format(xx.shape, yy.shape))\n",
    "\n",
    "        my_KNN(xx, yy)\n",
    "        my_Kmeans(xx, yy)\n",
    "\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f046e4",
   "metadata": {},
   "source": [
    "### debuge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac1313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fcc166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "504px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
